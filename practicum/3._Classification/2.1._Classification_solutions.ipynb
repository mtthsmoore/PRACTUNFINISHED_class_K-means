{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "2.1._Classification_solutions.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO24y4Jju21u",
        "colab_type": "text"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhcU9Uucu21v",
        "colab_type": "text"
      },
      "source": [
        "Most eukaryotic genes undergo intron splicing events. These happen at the level of mRNA, after transcription and before translation. Pre-mRNA consists of exons, which are the proteincoding regions, and introns, which are the intervening sections in between. As a base rule, the spliceosome cuts out introns, retaining the consecutive exons to form mature mRNA. This mature mRNA is then translated to protein.\n",
        "\n",
        "In this practical you will build a classification model for gene splice site prediction from DNA sequences. The vast majority of splice sites are characterized by the presence of specific dimers on the intronic side of the splice site: \"GT\" for donor and \"AG\" for acceptor sites. Yet, only about 0.1-1% of all \"GT\" and \"AG\" occurrences in the genome represent true splice sites. \n",
        "\n",
        "We will focus on acceptor site prediction. You are given the following data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrdBiQtAu21v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/Machine_Learning_course_UGent_D012554/master/practicum/3._Classification/acceptor_sites_dataset_train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lJt4Vopu212",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "76f0fe2f-8a41-4dd6-ca61-b566fc8c9184"
      },
      "source": [
        "data.head(5)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>TTTGAATTGTAGGTGTCCTGCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>TATTTTTTAAAGAACTGGAAGA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>TTTCTTTTTCAGATGAAGAATG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>TATTAATTTCAGTTTGGTTGTT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>TAAAAATTTAAGTTCGTCCCGA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                sequence\n",
              "0      1  TTTGAATTGTAGGTGTCCTGCT\n",
              "1      1  TATTTTTTAAAGAACTGGAAGA\n",
              "2      1  TTTCTTTTTCAGATGAAGAATG\n",
              "3      1  TATTAATTTCAGTTTGGTTGTT\n",
              "4      1  TAAAAATTTAAGTTCGTCCCGA"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXgIExxtu216",
        "colab_type": "text"
      },
      "source": [
        "There are only two columns. The column \"sequence\" contains a local DNA context sequence that surrounds a candidate acceptor site (nucleotides at positions 11 and 12 in the sequence are always \"A\" and \"G\" respectively), so these positions are candidate gene acceptor sites. The column \"label\" indicates the class: 1 for \"is acceptor site\" and -1 for \"is not acceptor site\". The goal is to predict the target from the local context sequence of the candidate acceptor site. \n",
        "\n",
        "*How many sequences does the dataset contain for each class?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYhNkKz3u217",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "84193258-9a81-46d1-8fca-4014107b11b4"
      },
      "source": [
        "data['label'].value_counts()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1    1503\n",
              " 1     145\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teweqTUhu21_",
        "colab_type": "text"
      },
      "source": [
        "Next, useful features need to be computed from the DNA sequences, a process known as **feature engineering**. \n",
        "\n",
        "The Pandas `.apply()` method that allows us the process the values in a DataFrame column to create a new column. We will apply this function to compute feature vectors from the `sequence` column in the `data` DataFrame.  \n",
        "\n",
        "The \"AG\" in the middle of each context sequence is the same for both classes, i.e. it does not provide any discriminative information. So, We shouldn't compute features from the middle AG dinculeotide in the local context sequence.\n",
        "\n",
        "*Use the Pandas DataFrame `.apply()` method to remove the middle \"AG\" dinucleotides in the DNA sequences (don't create a new column):*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8ThMz7Su22A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "9b76d1be-5d73-4dec-d49a-1855c199c689"
      },
      "source": [
        "def remove_AG(x):\n",
        "    return x[0:10]+x[12:22]\n",
        "\n",
        "print(data.head())\n",
        "\n",
        "data[\"sequence\"] = data[\"sequence\"].apply(remove_AG)\n",
        "\n",
        "print(data.head())"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   label                sequence\n",
            "0      1  TTTGAATTGTAGGTGTCCTGCT\n",
            "1      1  TATTTTTTAAAGAACTGGAAGA\n",
            "2      1  TTTCTTTTTCAGATGAAGAATG\n",
            "3      1  TATTAATTTCAGTTTGGTTGTT\n",
            "4      1  TAAAAATTTAAGTTCGTCCCGA\n",
            "   label              sequence\n",
            "0      1  TTTGAATTGTGTGTCCTGCT\n",
            "1      1  TATTTTTTAAAACTGGAAGA\n",
            "2      1  TTTCTTTTTCATGAAGAATG\n",
            "3      1  TATTAATTTCTTTGGTTGTT\n",
            "4      1  TAAAAATTTATTCGTCCCGA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZkQumNiu22E",
        "colab_type": "text"
      },
      "source": [
        "A trivial feature vector representation would be to replace each amino acid with a number, making the Machine Learning aware about each amino acid at each position in the DNA context sequence. The following method maps a DNA sequence `x`to a Python list with a number for each amino acid:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA5jYH1su22F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DNA_int_encoding(x):\n",
        "    encoding = []\n",
        "    for nuc in x:\n",
        "        if nuc == 'A':\n",
        "            encoding.append(0)\n",
        "        elif nuc == 'C':\n",
        "            encoding.append(1)\n",
        "        elif nuc == 'G':\n",
        "            encoding.append(2)\n",
        "        elif nuc == 'T':\n",
        "            encoding.append(3)\n",
        "        else:\n",
        "            print(\"Found non-nucleotide in %s\"%x)\n",
        "    return encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MJXJBj0u22I",
        "colab_type": "text"
      },
      "source": [
        "*Apply this function on the `sequence` column in the `data` DataFrame to create a Series with feature vectors (write the resulting feature vectors to a variable called `data_features_int_encoding`:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFRdEtCeu22J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a54e69b4-8c3a-4249-9286-cdc255aaec61"
      },
      "source": [
        "data_features_int_encoding = data[\"sequence\"].apply(DNA_int_encoding)\n",
        "\n",
        "data_features_int_encoding.head()"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [3, 3, 3, 2, 0, 0, 3, 3, 2, 3, 2, 3, 2, 3, 1, ...\n",
              "1    [3, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 1, 3, 2, ...\n",
              "2    [3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 0, 3, 2, 0, 0, ...\n",
              "3    [3, 0, 3, 3, 0, 0, 3, 3, 3, 1, 3, 3, 3, 2, 2, ...\n",
              "4    [3, 0, 0, 0, 0, 0, 3, 3, 3, 0, 3, 3, 1, 2, 3, ...\n",
              "Name: sequence, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVDjAmKZu22P",
        "colab_type": "text"
      },
      "source": [
        "Next we put these feature vectors back in a Pandas DataFrame as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztKXm9yYu22Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "54a41cb7-33fa-4002-f5fe-d5dd6bae774f"
      },
      "source": [
        "data_features_int_encoding = pd.DataFrame(data_features_int_encoding.tolist())\n",
        "\n",
        "data_features_int_encoding.head()"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19\n",
              "0  3  3  3  2  0  0  3  3  2  3   2   3   2   3   1   1   3   2   1   3\n",
              "1  3  0  3  3  3  3  3  3  0  0   0   0   1   3   2   2   0   0   2   0\n",
              "2  3  3  3  1  3  3  3  3  3  1   0   3   2   0   0   2   0   0   3   2\n",
              "3  3  0  3  3  0  0  3  3  3  1   3   3   3   2   2   3   3   2   3   3\n",
              "4  3  0  0  0  0  0  3  3  3  0   3   3   1   2   3   1   1   1   2   0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0573K2rRu22W",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the generalization performance of a logisitc regression model with hyperparameters $C=0.1$ on the data set `data_features_int_encoding` using 10-fold cross-validation. \n",
        "\n",
        "*Apply the `cross_val_score()` function to compute the mean accuracy of the CV-scores.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc7J8SrIu22X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6c8795b-c63e-416d-c496-3ddc2bd36794"
      },
      "source": [
        "# solution!!\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "model = LogisticRegression(C=0.1)\n",
        "print(np.mean(cross_val_score(model,data_features_int_encoding,data.label,cv=10)))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9253658536585366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0olCEtYau22e",
        "colab_type": "text"
      },
      "source": [
        "Next we load an independent test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv_NJz5Uu22g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_test = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/Machine_Learning_course_UGent_D012554/master/practicum/3._Classification/acceptor_sites_dataset_test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj9SshLnu22l",
        "colab_type": "text"
      },
      "source": [
        "*Compute the feature vectors for the test set:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtuUX8K8u22m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# solution !!!\n",
        "\n",
        "data_test[\"sequence\"] = data_test[\"sequence\"].apply(remove_AG)\n",
        "\n",
        "data_test_features_int_encoding = data_test[\"sequence\"].apply(DNA_int_encoding)\n",
        "data_test_features_int_encoding = pd.DataFrame(data_test_features_int_encoding.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9SWqTScu22u",
        "colab_type": "text"
      },
      "source": [
        "*Fit a logistic regression model on the train set.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHuDj5xsu22v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "cafb0997-a392-449a-b36c-5b211d7f8595"
      },
      "source": [
        "model.fit(data_features_int_encoding,data.label)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_gu0seZu221",
        "colab_type": "text"
      },
      "source": [
        "*Make model predictions for the test set.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzX1UJqlu222",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(data_test_features_int_encoding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNIux5gFu227",
        "colab_type": "text"
      },
      "source": [
        "Scikit-learn offers many metrics to evaluate model predictions. These functions are contained in the `metrics` module of `sklearn`. \n",
        "\n",
        "*Can you find how to compute the accuracy of these predictions (use the `metrics`module)?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTFGB05gu228",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a750bd70-a20a-4ee5-b324-d228cffd6ddb"
      },
      "source": [
        "#solution!!\n",
        "from sklearn import metrics\n",
        "\n",
        "print(metrics.accuracy_score(data_test.label,predictions))"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9221014492753623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPu8YDA2u23D",
        "colab_type": "text"
      },
      "source": [
        "An accuracy above 90% seems like a good score. But is it? Let's consider a model that predicts class \"-1\" for all test points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kV1yMLOu23E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_zero = [-1]*len(data_test.label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efNP_I-Ju23I",
        "colab_type": "text"
      },
      "source": [
        "*What is the accuracy of these predictions?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmdBsSaou23K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1694b136-1a3a-4a29-c381-615a74824a74"
      },
      "source": [
        "# solution !!\n",
        "print(metrics.accuracy_score(data_test.label,predictions_zero))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9003623188405797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lZ3nJUPu23P",
        "colab_type": "text"
      },
      "source": [
        "So this should be a good score as well, even though we did not learn anything.\n",
        "\n",
        "For classification tasks where the classes are highly imbalanced accuracy is not a good metric to evaluate the generalization performance. In fact, if there are 0.1% \"AG\" dinucleotides in a genome that are true acceptor sites then a model that predicts class \"-1\" for each \"AG\" would have an accuracy of 99.9%.\n",
        "\n",
        "We have seen how a ROC curve plots the true positives rate against the false positives rate. Both these metrics focus on the positive class, in our case the true acceptor sites. These metrics are much more suitable to evalute the performance of models on tasks with highly imbalanced classes. To transform a ROC curve into one metric we can use the area under the curve (AUC). \n",
        "\n",
        "*What is the AUC score of the predictions computed by the linear regression model we fitted?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTjM_o7Iu23Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5dff9240-57c4-46c9-dd68-98f82b0caa08"
      },
      "source": [
        "# solution !!\n",
        "print(metrics.auc(data_test.label,predictions))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbuR3_0Uu23V",
        "colab_type": "text"
      },
      "source": [
        "You should see a negative value. This is because to compute the AUC we need the predictions to be scores (a continuous value) rather than class labels (discrete values). \n",
        "\n",
        "For logistic regression these scores are the class probabilities predicted by the model. We can obtain them using the `predict_proba()` function of the `LogisticRegression` module as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMT2X0i7u23W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict_proba(data_test_features_int_encoding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOjXzC5fu23b",
        "colab_type": "text"
      },
      "source": [
        "What does variable `predictions` contain?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnYhvCSlu23c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "6ee7cefb-69a7-4168-caa9-779c62dcd62e"
      },
      "source": [
        "# solution !!\n",
        "predictions"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.33015439, 0.66984561],\n",
              "       [0.44977258, 0.55022742],\n",
              "       [0.34876393, 0.65123607],\n",
              "       ...,\n",
              "       [0.99674001, 0.00325999],\n",
              "       [0.9820754 , 0.0179246 ],\n",
              "       [0.982688  , 0.017312  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecMykZJcu23h",
        "colab_type": "text"
      },
      "source": [
        "The first and second column contain the predicted probabilities for class '-1' and '1' respectively. To compute the AUC we need to use the positive class probabilities. \n",
        "\n",
        "*What is the AUC now?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7Zj60JBu23i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4c6de89a-d6cc-4159-975a-f1c9a2e0ef3c"
      },
      "source": [
        "# solution !!\n",
        "print(metrics.auc(data_test.label,predictions[:,1]))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.365258280739163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EudhUPu-u23o",
        "colab_type": "text"
      },
      "source": [
        "Is this good generalization performance?\n",
        "\n",
        "Transforming categorical features into ordered integers is maybe not a good idea as the nucleotides don't have any ordering. It is better to transform a categorical feature into one binary feature for each category (known as *one-hot* encoding). \n",
        "\n",
        "We can do this with the following function that again computes feature vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q62ik5Gyu23p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DNA_onehot_encoding(x):\n",
        "    encoding = []\n",
        "    for nuc in x:\n",
        "        if nuc == 'A':\n",
        "            encoding.extend([1,0,0,0])\n",
        "        elif nuc == 'C':\n",
        "            encoding.extend([0,1,0,0])\n",
        "        elif nuc == 'G':\n",
        "            encoding.extend([0,0,1,0])\n",
        "        elif nuc == 'T':\n",
        "            encoding.extend([0,0,0,1])\n",
        "        else:\n",
        "            encoding.extend([0,0,0,0])\n",
        "    return encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufn2ity1u23y",
        "colab_type": "text"
      },
      "source": [
        "*Create a Pandas DataFrame called `data_features_onehot_encoding` that contains the one-hot encoded features.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d61H88bFu23z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_features_onehot_encoding = pd.DataFrame(data[\"sequence\"].apply(DNA_onehot_encoding).tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "funm5spBu234",
        "colab_type": "text"
      },
      "source": [
        "Show the first five rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYXJydsdu235",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "a825401a-7368-420f-f251-23a0e38a5901"
      },
      "source": [
        "data_features_onehot_encoding.head()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0   1   2   3   4   5   6   7   8   9   10  ...  69  70  71  72  73  74  75  76  77  78  79\n",
              "0   0   0   0   1   0   0   0   1   0   0   0  ...   0   1   0   0   1   0   0   0   0   0   1\n",
              "1   0   0   0   1   1   0   0   0   0   0   0  ...   0   0   0   0   0   1   0   1   0   0   0\n",
              "2   0   0   0   1   0   0   0   1   0   0   0  ...   0   0   0   0   0   0   1   0   0   1   0\n",
              "3   0   0   0   1   1   0   0   0   0   0   0  ...   0   1   0   0   0   0   1   0   0   0   1\n",
              "4   0   0   0   1   1   0   0   0   1   0   0  ...   1   0   0   0   0   1   0   1   0   0   0\n",
              "\n",
              "[5 rows x 80 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJgd7t3T7x2j",
        "colab_type": "text"
      },
      "source": [
        "Now let's add interpretable feature names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Nszf4267siU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = []\n",
        "for i in range(-10,0,1):\n",
        "    for nuc in ['A','C','G','T']:\n",
        "        columns.append(\"%i_%s\"%(i,nuc))\n",
        "for i in range(1,11,1):\n",
        "    for nuc in ['A','C','G','T']:\n",
        "        columns.append(\"%i_%s\"%(i,nuc))       \n",
        "data_features_onehot_encoding.columns = columns\n",
        "data_features_onehot_encoding.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfAOG8SBu24E",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the generalization performance of a logisitc regression model with hyperparameters $C=0.1$ on the data set `data_features_onehot_encoding` using 10-fold cross-validation. \n",
        "The `cross_val_score()` has a function parameter called `scoring` that allows you to set different scoring metrics.\n",
        "\n",
        "*Use the `cross_val_score()` function to compute the mean AUC of the CV-scores.* \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq2cZVNSu24F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c48e0bbc-8803-4b4e-d3c5-adabf06e0cad"
      },
      "source": [
        "model = LogisticRegression(C=0.1)\n",
        "print(np.mean(cross_val_score(model,data_features_onehot_encoding,data.label,cv=10,scoring=\"roc_auc\")))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9867797540208135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqh-I3zuu24L",
        "colab_type": "text"
      },
      "source": [
        "*What is the AUC on `data_test`?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeCkxuubu24M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3e96bf71-5c16-4f5b-c8e8-0755d2de8f4c"
      },
      "source": [
        "data_test_features_onehot_encoding = pd.DataFrame(data_test[\"sequence\"].apply(DNA_onehot_encoding).tolist())\n",
        "\n",
        "model.fit(data_features_onehot_encoding,data.label)\n",
        "\n",
        "predictions = model.predict_proba(data_test_features_onehot_encoding)\n",
        "print(\"AUC=%.2f\" % metrics.auc(data_test.label,predictions[:,1]))"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC=0.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w-jdU-gu24Q",
        "colab_type": "text"
      },
      "source": [
        "Is this close to what your CV is telling you?\n",
        "\n",
        "We have used hyperparameter $C=0.1$ for the logistic regression model. \n",
        "\n",
        "*Is there a better value for this regularization parameter (use `GridSearchCV`)?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B63LA0zwu24S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "outputId": "97368290-a4fc-4b9a-f1a7-83103389e15b"
      },
      "source": [
        "#Solution !!\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "search_space = [0.001,0.01,0.1,1,10,100]\n",
        "\n",
        "params = dict(C=search_space)\n",
        "grid_search = GridSearchCV(model, param_grid=params)\n",
        "\n",
        "grid_search.fit(data_features_onehot_encoding,data.label)\n",
        "\n",
        "print(grid_search.best_estimator_)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjoFP1_bu24W",
        "colab_type": "text"
      },
      "source": [
        "*What is the 10-CV AUC performance with this value for $C$?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWxVj0hRu24X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9ec10cc2-124e-41ed-c383-cf44963c4755"
      },
      "source": [
        "model = LogisticRegression(C=1)\n",
        "print(np.mean(cross_val_score(model,data_features_onehot_encoding,data.label,cv=10,scoring=\"roc_auc\")))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9847463891516872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytpwmh2wu24c",
        "colab_type": "text"
      },
      "source": [
        "*What is the AUC performance on the test set for this value of $C$?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5906bHvu24d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ad2c9e1-edba-4fb0-927a-c94cb9f48590"
      },
      "source": [
        "model.fit(data_features_onehot_encoding,data.label)\n",
        "\n",
        "predictions = model.predict_proba(data_test_features_onehot_encoding)\n",
        "print(\"AUC=%.2f\" % metrics.auc(data_test.label,predictions[:,1]))"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC=0.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "wRe1CLnUu24l",
        "colab_type": "text"
      },
      "source": [
        "Is this closer to the AUC you computed using 10-CV?\n",
        "\n",
        "Let's see how the model is making predictions (what patterns it extracted). In scikit-learn a fitted model has its modelparameters stored in `.coef_[0]`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f-NBEUt4AjE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "5055679d-3335-42ee-cf15-2c329559f37a"
      },
      "source": [
        "print(model.coef_[0])"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.47913801 -0.21734157 -0.84815424  0.58622429  0.32255299 -0.38610416\n",
            " -0.73300594  0.79642359  0.43271421 -0.17206379 -0.82147414  0.5606902\n",
            "  0.11079628 -0.43093542 -0.31934691  0.63935255  0.86601068 -1.25349266\n",
            " -0.63738888  1.02473735 -0.11861092  0.06926326 -0.70450688  0.75372103\n",
            " -0.4718082  -0.6467675  -0.91722143  2.03566361 -1.27747723 -0.55827664\n",
            " -0.8625858   2.69820616 -0.11954326 -0.47204416 -0.84161892  1.43307283\n",
            " -0.64123723  2.05912016 -1.72034224  0.3023258   0.44369177 -0.72487884\n",
            "  1.11200797 -0.83095441 -0.07735498 -0.10416085  0.25928459 -0.07790228\n",
            "  0.03041843  0.1905419   0.2637964  -0.48489024 -0.32530546  0.44472124\n",
            "  0.26691915 -0.38646844 -0.66201988  0.57906414  0.50459143 -0.42176919\n",
            " -0.23317418  0.42568974  0.46338059 -0.65602966 -0.62006953  0.33864329\n",
            "  0.587697   -0.30640428 -0.14253564  0.225374    0.3280646  -0.41103648\n",
            " -0.07642458  0.51808758 -0.30303357 -0.13876294 -0.57864226  0.116347\n",
            "  0.69097627 -0.22881452]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdPAICuc4NWT",
        "colab_type": "text"
      },
      "source": [
        "For logistic regression this is one modelparameter for each feature (plus the interecept, which is not in `.coef_[0]`). \n",
        "\n",
        "Recall that for logistic regression a prediction is made by multiplying each fitted modelparameter with the corresponding feature, summing them and then squeezing this sum between 0 and 1 through the logistic function. Since all feature have values 0 or 1 the modelparameter values indicate the importance of a feature during prediction.\n",
        "\n",
        "For plotting we will put the feature names and modelparameter values in a new DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mnte18-u242",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "F_importances = []\n",
        "for feature_name, modelparameter in zip(data_features_onehot_encoding.columns,model.coef_[0]):\n",
        "    F_importances.append([feature_name,modelparameter])\n",
        "F_importances = pd.DataFrame(F_importances,columns=[\"feature_name\",\"importance\"])\n",
        "F_importances.head()    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SwWB_xQ8w8g",
        "colab_type": "text"
      },
      "source": [
        "*Use the Seaborn `.barplot()` method to plot this DataFrame:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_thWFCY894M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "3f6e850f-cf57-4b8c-f5c8-10f98fd197b3"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(18,8))\n",
        "chart = sns.barplot(x=\"feature_name\",y=\"importance\",data=F_importances)\n",
        "chart.set_xticklabels(chart.get_xticklabels(), rotation=90, horizontalalignment='right')\n",
        "plt.show()"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABB4AAAH0CAYAAACaQe5wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 1296x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0N9sbu79xoi",
        "colab_type": "text"
      },
      "source": [
        "*Can you make a better plot?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntsnD5Kwu25B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "5c293c54-b3aa-4f33-98b1-5f045481690f"
      },
      "source": [
        "def get_nuc(x):\n",
        "    return(x.split(\"_\")[1])\n",
        "\n",
        "def get_position(x):\n",
        "    if x.split(\"_\")[0] == \"A\": return 0\n",
        "    if x.split(\"_\")[0] == \"G\": return 0\n",
        "    return(int(x.split(\"_\")[0]))\n",
        "\n",
        "F_importances[\"nuc\"] = F_importances[\"feature_name\"].apply(get_nuc)\n",
        "F_importances[\"position\"] = F_importances[\"feature_name\"].apply(get_position)\n",
        "\n",
        "F_importances.head()"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_name</th>\n",
              "      <th>importance</th>\n",
              "      <th>nuc</th>\n",
              "      <th>position</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-10_A</td>\n",
              "      <td>0.479138</td>\n",
              "      <td>A</td>\n",
              "      <td>-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-10_C</td>\n",
              "      <td>-0.217342</td>\n",
              "      <td>C</td>\n",
              "      <td>-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-10_G</td>\n",
              "      <td>-0.848154</td>\n",
              "      <td>G</td>\n",
              "      <td>-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-10_T</td>\n",
              "      <td>0.586224</td>\n",
              "      <td>T</td>\n",
              "      <td>-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-9_A</td>\n",
              "      <td>0.322553</td>\n",
              "      <td>A</td>\n",
              "      <td>-9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  feature_name  importance nuc  position\n",
              "0        -10_A    0.479138   A       -10\n",
              "1        -10_C   -0.217342   C       -10\n",
              "2        -10_G   -0.848154   G       -10\n",
              "3        -10_T    0.586224   T       -10\n",
              "4         -9_A    0.322553   A        -9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5haGvAHu25c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "outputId": "0e2d347f-bfb2-4625-9622-55e044312f37"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "plt.figure(figsize=(18,8))\n",
        "sns.barplot(x=\"position\",y=\"importance\",hue=\"nuc\",data=F_importances)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa9b9014be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABB4AAAHgCAYAAAACBq79AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1296x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1uafole2Y-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}